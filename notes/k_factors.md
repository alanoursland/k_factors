# K-Factors algorithm

## 1. Problem Statement

Given

* a data matrix $X=\{x_1,\dots,x_n\}\subset\R^d$,
* the number of clusters $K$,
* and a target subspace dimension $R$ per cluster,

we wish to partition the points into $K$ clusters and, for each cluster $k$, extract an **orthonormal basis** $\{v_k^{(1)},\dots,v_k^{(R)}\}\subset\R^d$ (plus a mean $\mu_k$) so that each point incrementally â€œclaimsâ€ one dimension that best explains its residual variance.  

Once a point has claimed a direction, it contributes **less (down to zero)** to future directions that overlap with the claimed one, and **fully** to directions that are orthogonal.

---

## 2. Algorithm Overview

At each **stage** $t=1,\dots,R$ we:

1. **Assign** each point $x_i$ to the cluster whose current subspace best explains its residual variance.  
   (Assignments themselves are based on residual distance, not on the penalties.)
2. **Update** each clusterâ€™s mean $\mu_k$ by averaging its assigned points.
3. **Extract** the new direction $v_k^{(t)}$ by doing a rank-1 PCA on the **residuals**, weighting each point by a penalty that reduces its contribution if it has already claimed a similar direction.
4. **Record** for each point which direction it claimed, so we can penalize re-use in later stages.

---

## 3. Pseudocode

```plaintext
Input: X âˆˆ â„^{nÃ—d},  K,  R
Output:  Î¼â‚â€¦Î¼_K âˆˆ â„^d;  for each k,  {v_k^(1)â€¦v_k^(R)} orthonormal in â„^d;
        and per-point claimed directions D_i = [d_iÂ¹â€¦d_i^R]

Initialize all Î¼_k (e.g. by K-means centroids)
For all i,  set D_iâ†empty list

for t = 1 to R:
  // â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
  // 1) (Hard) Assignment Step
  for i in 1â€¦n:
    for k in 1â€¦K:
      // residual projector after removing old directions in cluster k
      P_k^{(<t)} = âˆ‘_{u=1}^{t-1} v_k^{(u)} v_k^{(u)áµ€}
      r_{ik} = (I âˆ’ P_k^{(<t)}) (x_i âˆ’ Î¼_k)
      cost_{ik} = â€–r_{ik}â€–Â²
    end
    (k_i,â„“_i) = argmin_{k} cost_{ik}          // â„“_i â‰¡ t implicitly
    assign x_i â†’ cluster k_i
  end

  // 2) Update Means
  for k in 1â€¦K:
    let ğ’³_k = { x_i : i assigned to k }
    Î¼_k â† (1/|ğ’³_k|) âˆ‘_{xâˆˆğ’³_k} x
  end

  // 3) Sequential PCA on Residuals â†’ new v_k^(t)
  for k in 1â€¦K:
    Form residuals for assigned points:
      y_i = x_i âˆ’ Î¼_k
      r_i = (I âˆ’ âˆ‘_{u=1}^{t-1} v_k^(u) v_k^(u)áµ€ ) y_i

    Compute penalty weights:
      w_i = âˆ_{dâˆˆD_i} (1 âˆ’ |v_k^{(t)}Â·d|)
      // 1.0 if orthogonal, 0.0 if parallel

    Form weighted scatter S_k = âˆ‘_{i:assigned} w_i Â· (r_i r_iáµ€)

    v_k^(t) = top eigenvector of S_k   // unitâ€norm
  end

  // 4) Record claimed directions
  for each point x_i:
    append d_i^t â† v_{k_i}^{(t)}  to D_i
  end
end
````

---

## 4. Time Complexity

Let \$n\$ = #points, \$d\$ = ambient dimension, \$K\$ = #clusters, \$R\$ = subspace dimension per cluster, and assume we run **one assignment + mean-update** cycle per stage (for simplicity).

1. **Assignment step** per stage \$t\$:

   * For each pointâ€“cluster pair \$(i,k)\$:

     * Compute residual \$r\_{ik}\$: projecting out \$t-1\$ directions costs \$O(t,d)\$.
     * Squared norm: \$O(d)\$.
   * Total per stage:

     $$
       O\bigl(n\;K\;(t\,d)\bigr)
       \;\le\;
       O(n\,K\,d\,R).
     $$

2. **Mean update** per stage:

   $$
     O(n\,d).
   $$

3. **Sequential PCA** per cluster per stage:

   * Weighted residual scatter: \$O(|\mathcal X\_k|,d,t)\$, summing over \$k\$ gives \$O(n,d,t)\$.
   * Topâ€eigenvector of a \$d\times d\$ matrix: \$O(d^2)\$ per power-iteration; constant number â‡’ \$O(d^2)\$.
   * Over \$K\$ clusters:

     $$
       O(n\,d\,t \;+\; K\,d^2)
       \;\le\;
       O(n\,d\,R + K\,d^2).
     $$

Summing across \$t=1\ldots R\$, the **total** time is on the order of

$$
\sum_{t=1}^R \Bigl[O(n\,K\,d\,t) + O(n\,d) + O(n\,d\,t + K\,d^2)\Bigr]
\;=\;
O\bigl(n\,K\,d\,R^2 + K\,d^2\,R + n\,d\,R\bigl).
$$

In practice \$R\ll d\$ or \$R\ll K\$, so the dominant costs are
\$;O(n,K,d,R^2)\$ for assignment and
\$;O(K,d^2,R)\$ for eigen-computations.

---

## 5. Space Complexity

* **Data matrix** \$X\$: \$O(n,d)\$.
* **Cluster means** \${\mu\_k}\$: \$O(K,d)\$.
* **Directions** \${v\_k^{(u)}}\$: \$O(K,d,R)\$.
* **Point-wise claimed lists** \$D\_i\$: storing \$t\$ unit-vectors per point â‡’ \$O(n,d,R)\$.
* **Temporary residuals/scatters**: \$O(d^2)\$ per cluster.

Overall:

$$
O\bigl(n\,d \;+\; K\,d\,R \;+\; n\,d\,R\bigr).
$$

---

### Remarks

* This algorithm is **block-coordinate-descent** on a well-defined objective (hard Mixture of PPCA), so it **monotonically decreases** that objective.
* In the special case \$R=1\$ it reduces to the standard **K-Lines / K-Subspaces** algorithm.
* When soft assignments replace the hard arg-mins, you recover **Mixtures of Probabilistic PCA** (an EM algorithm).

